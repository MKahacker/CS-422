{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tan ch 1**\n",
    "\n",
    "Objects == Records <br/>\n",
    "Attributes == Features <br/>\n",
    "<br>\n",
    "Types of Data :- \n",
    "- Nominal (no function except identification) Red, Yellow, Blue. Ids. Names.\n",
    "- Ordinal (Descriptive labels that can be ordered) Brightest, Bright, Dark.  \n",
    "- Interval (Labels that have meaningful difference) Temperature. Calendar Days. \n",
    "- Ratio (Numbers that have meaningful distances and ratios) All other numbers\n",
    "\n",
    "Every type includes every types before its characteristics.\n",
    "\n",
    "Categorical is Nominal and Ordinal. Numerical is Interval and Ratio.\n",
    "\n",
    "Asymmetric Attributes :- Attributes where only presence matters.\n",
    "\n",
    "Critiques :- \n",
    "- Asymmetric Binary <br>\n",
    "- Cyclical <br>\n",
    "- Multivariate <br>\n",
    "- Partially ordered <br>\n",
    "- Partial membership <br>\n",
    "- Relationships between the data \n",
    "\n",
    "Real data is approximate and noisy\n",
    "\n",
    "Dataset types :- \n",
    "- Record\n",
    " - Data Matrix\n",
    " - Document Data (Matrix with words as features)\n",
    " - Transaction Data (Matrix with items as features)\n",
    "- Graph\n",
    " - Worldwide Web\n",
    " - Molecular Structure\n",
    "- Ordered\n",
    " - Spatial Data\n",
    " - Temporal Data\n",
    " - Sequential Data\n",
    " - Genetic Sequence\n",
    " \n",
    "\n",
    "Data Quality :- the quality of data determines how good the model will be. As an analyst you cannot control the quality of your data but you can perform pre-processing to make your data cleaner.\n",
    "\n",
    "Data Quality Problems :- \n",
    "- Noise (non relevant data)\n",
    "- Outliers (data that is in the extremes)\n",
    "- Missing Values (blank values)\n",
    "- Duplicate data (attributes or objects that are the same)\n",
    "- Wrong data (erroneous data)\n",
    "\n",
    "Missing Value Types :- \n",
    "- MCAR (missing value is independent)\n",
    " - Missingness of a value is independent of attributes\n",
    " - Fill in values based on the attributes\n",
    " - Analysis may be biased overall\n",
    "- MAR (missing value is used in other variables)\n",
    " - Missingness is related to other variables\n",
    " - Fill in values based other values\n",
    " - Almost always produces a bias in the analysis\n",
    "- MNAR (missing value is related to unknown variable)\n",
    " - Missingness is related to unobserved measurements\n",
    " - Informative and non-ignorable missingness.\n",
    "\n",
    "Similarity - Difference :-\n",
    "- Similarity (Attributes)\n",
    " - Nominal (==) is 1 (!=) is 0\n",
    " - Ordinal $1 - d$\n",
    " - Interval, Ratio s = -d  \n",
    "- Disimilarity (Attributes)\n",
    " - Nominal (==) is 0 (!=) is 1\n",
    " - Ordinal $\\frac{|x - y|}{n-1}$ we do this because we want to establish a distance between variables.\n",
    " - Interval/Ratio |x - y| \n",
    "\n",
    "Distance (Objects) :- \n",
    " - Eucalidian Distance = $ \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $\n",
    " - SMC = $\\frac{f_{11} + f_{00}}{f_{11} + f_{10} + f_{01} + f_{00}}$\n",
    " - Jaccard = $\\frac{f_{11}}{f_{11} + f_{10} + f_{01}}$\n",
    " - Cosine = $\\frac{<x \\cdot y>}{|x| * |y|}$\n",
    " - Hamming = $\\sum_{i=1}^{n} (x_i - y_i)$\n",
    " - Minkowski difference = $ (\\sum_{i=1}^{n} |x_i - y_i|^r)^{1/r}$\n",
    "\n",
    "\n",
    "Distances have common properties :-\n",
    " - $d(x,y) >= 0$\n",
    " - Symmetry: $d(x,y) = d(y,x)$\n",
    " - Triangle Inequality: $ d(x,z) <= d(x,y) + d(y,z)$\n",
    " \n",
    "**Covariance** = $$s_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) $$\n",
    "\n",
    "**Standard Deviation** = $$s_x = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n",
    "\n",
    "**Correlation** = $$\\frac{s_{xy}}{s_x*s_y}$$\n",
    "\n",
    "Data Preprocessing :- Removing duplicates, useless data, preparing your data, dealing with missing values.\n",
    "\n",
    " \n",
    "**Entropy** = $$H(x) = - \\sum_{i=1}^{n} p_i\\log_2p_i$$\n",
    "\n",
    "Data Preprocessing :-\n",
    "- Aggregation (combining two attributes to into a single attribute) p(x,y,z) = a\n",
    " - Data Reduction\n",
    " - Change of scale\n",
    " - More \"stable\" data\n",
    "- Sampling (taking a smaller set of the population)\n",
    " - Sampling is as good as population if representative\n",
    " - Simple Random Sampling (select random objects, no way to see if representative)\n",
    " - Stratified Sampling (partition population and then select an amount from each section)\n",
    "\n",
    "Dimensionality Curse :- The higher the dimesionality of the dataset the more sparse it becomes. This is because the dimensionality individualizes the objects more. \n",
    "\n",
    "Dimensionality Reduction :- reduces dimesionality while perserving variance\n",
    "- Avoids curse of dimensionality\n",
    "- Save memory and computation time\n",
    "- May help reduce noise and outliers\n",
    "\n",
    "Techniques for dim red :- \n",
    "- Feature selection (selecting \"good\" data)\n",
    "- PCA \n",
    "- Singular Value Decomposition\n",
    "- Suprevised and non-linear dim red\n",
    "- Feature creation (creating a feature our of other features)\n",
    "- Mapping onto new space\n",
    "\n",
    "PCA :-\n",
    "goal - look for a direction that captures the most variance in the dataset.\n",
    "\n",
    "Steps for PCA:\n",
    "- Center data minus all points by the mean\n",
    "- Compute covariance matrix, look at how all data points correlate\n",
    " - Multiplying by the covariance matrix will turn a vector to the direction with the greatest variance in the dataset.\n",
    " - We can use this to find the eigen vector or the vector that the covariance turns to\n",
    "\n",
    "\n",
    "$$ \\sum e = \\lambda e $$\n",
    "**Matrix** ($\\sum$)\n",
    "\n",
    "|dim1|dim2|\n",
    "|----|----|\n",
    "|x1  | x2 |\n",
    "|x3  | x4 |\n",
    "\n",
    "- find eigen values (take covariance matrix $\\sum$)\n",
    " - $det(\\sum - \\lambda I) = 0$\n",
    " - solving the above equation gives us eigen values.\n",
    " - $det(\\sum - \\lambda I) = (x_1 - \\lambda)(x_4 - \\lambda)(x_2)(x_3)= 0$\n",
    " - solve for $\\lambda_1,\\lambda_2$\n",
    " - find eigenvector $\\sum e_i = \\lambda_i e_i$\n",
    "  - $x_1*e_{i,1} + x_2*e_{i,2} = \\lambda_i*e_{i,1}$\n",
    "  - $x_3*e_{i,1} + x_4*e_{i,2} = \\lambda_i*e_{i,2}$\n",
    " - Solving gives you eigen vector for first eigen value.\n",
    " - Continue till you found eigen vector for each eigen value.\n",
    " \n",
    "eigen values = variance along eigen vector.\n",
    "\n",
    "Discretization :- making a continous number discreet by dividing into ranges\n",
    "- Mainly used in classification\n",
    "\n",
    "Binarization :- making a continous number binary by using ranges\n",
    "\n",
    "Attribute transform :- function that maps original to a new set\n",
    "- Normalization is a transform.\n",
    "\n",
    "PC1 maximizes the variance and minmizes **mean squared error**\n",
    "\n",
    "$$ var(A) = \\sum_{i=1}^{n} \\lambda_i$$\n",
    "\n",
    "**error** = $x - \\hat{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zaki ch 2**\n",
    "\n",
    "Mode = most frequent\n",
    "\n",
    "Median = middle of data density function f(m) = 0.5\n",
    "\n",
    "Range = max(x) - min (x)\n",
    "\n",
    "IQR = $F^{-1}(0.75) - F^{-1}(0.25)$\n",
    "\n",
    "Total variance: $var(D) = \\sigma_1^2 + \\sigma_2^2$ \n",
    "\n",
    "Relative variance: $det(D) = \\sigma_1^2 \\sigma_2^2 - \\sigma_{12}^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zaki ch 6**\n",
    "\n",
    "Hyper-rectangle :- \n",
    "\n",
    "**R** = $\\prod_{i=1}^{d} [min(x_i), max(x_i)]$ the min and max specify the range\n",
    "\n",
    "**Hypercube** :-\n",
    "\n",
    "$m = max_{j=1}^{d}max_{i=1}^{n}(|x_{ij}|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tan ch 3**\n",
    "\n",
    "Every dataset has a tuple (x, y) where x is the data and y is the class label\n",
    "\n",
    "We can divided our data into **training set** and **test set**. Each set has the (x, y) tuple.\n",
    "\n",
    "**Classifier** :-\n",
    "- Decision Trees\n",
    "- Rule-based methods\n",
    "- Nearest-neighbor\n",
    "- Neural Networks\n",
    "- Deep learning\n",
    "- Bayes\n",
    "- Support Vector Machines\n",
    "\n",
    "**Ensemble Classifiers** :- collection of classifiers\n",
    "- Boosting\n",
    "- Bagging\n",
    "- Random forest\n",
    "\n",
    "**Hunt's Alg** :- one of the oldest tree algorithms. If all test data belongs to the same class than it is a leaf node. Otherwise we have to split the node. We just keep splitting the node.\n",
    "\n",
    "**CART** :- Another tree algorithm which uses the gini index to develop trees.\n",
    "\n",
    "How do we determine test conditions?\n",
    "\n",
    "It depends on the value also depends on how many splits.\n",
    "\n",
    "Tree data types :-\n",
    "- Binary\n",
    "- Nominal\n",
    "- Ordinal\n",
    "- Continous \n",
    "\n",
    "Different splits :-\n",
    "- 2-way splits\n",
    "- Multiway splits\n",
    "\n",
    "Discretization :- \n",
    "- Static (you do the discretization in the beginning)\n",
    "- Dynamic (divide at every node)\n",
    "\n",
    "**Greedy Algorithm** :-\n",
    "- The algorithm chooses the purest nodes. \n",
    "\n",
    "**Gini(t)** = $$ 1 - \\sum_{i=1}^{n}p_i^2 $$\n",
    "\n",
    "**Entropy(t)** = $$ - \\sum_{i=1}^{n}p_i\\log_2 p_i $$\n",
    "\n",
    "**Misclassification(t)** = $$ 1 - max(p_i) $$\n",
    "\n",
    "**Gain** = $$i(parent)- i(split)$$\n",
    "\n",
    "**Gain Ratio** = $$ \\frac{Gain}{entropy(t)}$$\n",
    "\n",
    "**Gini split** = $$\\sum_{i=1}^{k}\\frac{n_i}{n}Gini(i)$$\n",
    "\n",
    "**Accuracy** = 1 - error\n",
    "\n",
    "**Error rate** = $$\\frac{FP + FN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Accuracy** = $$\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Gain Split** = $$ Entropy(parent) - \\sum_{i = 1}^{k} \\frac{n_i}{n} Entropy(i) $$ \n",
    "\n",
    "**Gain Ratio** = $$ \\frac{Gain Split}{-\\sum_{i=1}^{k} \\frac{n_i}{n}\\log_2(\\frac{n_i}{n})} $$\n",
    "\n",
    "**Precision** = $$ \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Recall** = $$ \\frac {TP}{TP + FN} $$\n",
    "\n",
    "$\\alpha $= FP  know as Type I error <br>\n",
    "$\\beta $= FN  know as Type II error \n",
    "\n",
    "**Decision Trees** :- \n",
    "- Advantages\n",
    " - Inexpensive to make\n",
    " - fast at classifying\n",
    " - Easy to interpert\n",
    " - robust to noise\n",
    " - can easily handle redundant data\n",
    "- Disadvantages\n",
    " - There are many trees to choose from\n",
    " - Doesn't take into account interactions with attributes\n",
    " - Decision boundaries involve single attributes\n",
    " \n",
    "Decision trees are a set of rules comprising a decision path to a leaf.\n",
    "\n",
    "Decision trees are made by looking at a data set and choosing an attribute and split that would give us the most information gain. Once we find that value we split the tree and put all data that fulfills the split condition into one side and all the values that fail the condition on the other side. We do this recursively with each sub tree until we reach the minimum leaf size or our leaves purity reaches a certain threshold.\n",
    "\n",
    "We choose a split by using split entropy, gini or misclassification.\n",
    "\n",
    "$$ \\textbf{Error rate} = \\frac{1}{n} \\sum_{i=1}^n I(y_i \\neq \\hat{y_i})$$ \n",
    "$$ \\textbf{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n I(y_i = \\hat{y_i}) = 1 - \\textbf{Error rate}$$\n",
    "$$ \\textbf{Precision} = \\frac{TP}{TP+FP} $$\n",
    "$$ \\textbf{Coverage} = \\textbf{Recall} = \\textbf{TPR}=\\frac{TP}{TP+FN} $$\n",
    "$$ \\textbf{TNR} = \\frac{\\textbf{TN}}{\\textbf{FP} + \\textbf{TN}} $$\n",
    "$$ \\textbf{FPR} = \\frac{\\textbf{FP}}{\\textbf{FP} + \\textbf{TN}} $$\n",
    "$$ \\textbf{F1-score} = \\frac{1}{\\frac{1}{\\textbf{precision}} + \\frac{1}{\\textbf{recall}}} = \\frac{2 \\cdot \\textbf{precision}\\cdot \\textbf{recall}}{\\textbf{precision} + \\textbf{recall}}$$\n",
    "\n",
    "**Contingency Table** :-\n",
    "\n",
    "|                   |**True Value** |       |\n",
    "|-------------------|---------------|-------|\n",
    "|**Predicted Value**|*True*         |*False*|\n",
    "|*True*             |TP             | FP    |\n",
    "|*False*            |FN             | TN    |\n",
    "\n",
    "**ROC Analysis** :- Uses FPR and TPR to determine how good a classifier is . A good classifier has a high TPR and a low FPR.\n",
    "\n",
    "-----------------------------------------------START FROM K-FOLD --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X⊆Y, then sup(X)≥sup(Y), which leads to the following two observations: (1) if X is frequent, then any subset Y ⊆ X is also frequent, and(2) if X is not frequent, then any superset Y ⊇ X cannot be frequent.The Apriori algorithm utilizes these two properties to significantly improve the brute-force approach. It employs a level-wise or breadth-first exploration of the itemset search space, and prunes all supersets of any infrequent candidate, as no superset of an infrequent itemset can be frequent. It also avoids generating any candidate that has an infrequent subset.In addition to improving the candidate generation step via itemset pruning, the Apriori method also significantly improves the I/O complexity. Instead ofcounting the support for a single itemset, it explores the prefix tree in a breadth-first manner, and computes the support of all the valid candidates ofsize k that comprise level k in the prefix tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
